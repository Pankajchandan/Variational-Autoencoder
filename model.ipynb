{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from input_pipeline import input_pipeline\n",
    "from input_pipeline import batch_norm\n",
    "import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VAE(input_shape=[None, 784],\n",
    "        n_filters=[64, 64, 64],\n",
    "        filter_sizes=[4, 4, 4],\n",
    "        n_hidden=32,\n",
    "        n_code=2,\n",
    "        activation=tf.nn.tanh,\n",
    "        dropout=False,\n",
    "        denoising=False,\n",
    "        convolutional=False,\n",
    "        variational=False):\n",
    "    \"\"\"(Variational) (Convolutional) (Denoising) Autoencoder.\n",
    "\n",
    "    Uses tied weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : list, optional\n",
    "        Shape of the input to the network. e.g. for MNIST: [None, 784].\n",
    "    n_filters : list, optional\n",
    "        Number of filters for each layer.\n",
    "        If convolutional=True, this refers to the total number of output\n",
    "        filters to create for each layer, with each layer's number of output\n",
    "        filters as a list.\n",
    "        If convolutional=False, then this refers to the total number of neurons\n",
    "        for each layer in a fully connected network.\n",
    "    filter_sizes : list, optional\n",
    "        Only applied when convolutional=True.  This refers to the ksize (height\n",
    "        and width) of each convolutional layer.\n",
    "    n_hidden : int, optional\n",
    "        Only applied when variational=True.  This refers to the first fully\n",
    "        connected layer prior to the variational embedding, directly after\n",
    "        the encoding.  After the variational embedding, another fully connected\n",
    "        layer is created with the same size prior to decoding.  Set to 0 to\n",
    "        not use an additional hidden layer.\n",
    "    n_code : int, optional\n",
    "        Only applied when variational=True.  This refers to the number of\n",
    "        latent Gaussians to sample for creating the inner most encoding.\n",
    "    activation : function, optional\n",
    "        Activation function to apply to each layer, e.g. tf.nn.relu\n",
    "    dropout : bool, optional\n",
    "        Whether or not to apply dropout.  If using dropout, you must feed a\n",
    "        value for 'keep_prob', as returned in the dictionary.  1.0 means no\n",
    "        dropout is used.  0.0 means every connection is dropped.  Sensible\n",
    "        values are between 0.5-0.8.\n",
    "    denoising : bool, optional\n",
    "        Whether or not to apply denoising.  If using denoising, you must feed a\n",
    "        value for 'corrupt_prob', as returned in the dictionary.  1.0 means no\n",
    "        corruption is used.  0.0 means every feature is corrupted.  Sensible\n",
    "        values are between 0.5-0.8.\n",
    "    convolutional : bool, optional\n",
    "        Whether or not to use a convolutional network or else a fully connected\n",
    "        network will be created.  This effects the n_filters parameter's\n",
    "        meaning.\n",
    "    variational : bool, optional\n",
    "        Whether or not to create a variational embedding layer.  This will\n",
    "        create a fully connected layer after the encoding, if `n_hidden` is\n",
    "        greater than 0, then will create a multivariate gaussian sampling\n",
    "        layer, then another fully connected layer.  The size of the fully\n",
    "        connected layers are determined by `n_hidden`, and the size of the\n",
    "        sampling layer is determined by `n_code`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : dict\n",
    "        {\n",
    "            'cost': Tensor to optimize.\n",
    "            'Ws': All weights of the encoder.\n",
    "            'x': Input Placeholder\n",
    "            'z': Inner most encoding Tensor (latent features)\n",
    "            'y': Reconstruction of the Decoder\n",
    "            'keep_prob': Amount to keep when using Dropout\n",
    "            'corrupt_prob': Amount to corrupt when using Denoising\n",
    "            'train': Set to True when training/Applies to Batch Normalization.\n",
    "        }\n",
    "    \"\"\"\n",
    "    # network input / placeholders for train (bn) and dropout\n",
    "    x = tf.placeholder(tf.float32, input_shape, 'x')\n",
    "    phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    corrupt_prob = tf.placeholder(tf.float32, [1])\n",
    "\n",
    "    # apply noise if denoising\n",
    "    x_ = (library.corrupt(x) * corrupt_prob + x * (1 - corrupt_prob)) if denoising else x\n",
    "\n",
    "    # 2d -> 4d if convolution\n",
    "    x_tensor = library.to_tensor(x_) if convolutional else x_\n",
    "    current_input = x_tensor\n",
    "\n",
    "    Ws = []\n",
    "    shapes = []\n",
    "\n",
    "    # Build the encoder\n",
    "    for layer_i, n_output in enumerate(n_filters):\n",
    "        with tf.variable_scope('encoder/{}'.format(layer_i)):\n",
    "            shapes.append(current_input.get_shape().as_list())\n",
    "            if convolutional:\n",
    "                h, W = library.conv2d(x=current_input,\n",
    "                                    n_output=n_output,\n",
    "                                    k_h=filter_sizes[layer_i],\n",
    "                                    k_w=filter_sizes[layer_i])\n",
    "            else:\n",
    "                h, W = library.linear(x=current_input,\n",
    "                                    n_output=n_output)\n",
    "            h = activation(batch_norm(h, phase_train, 'bn' + str(layer_i)))\n",
    "            if dropout:\n",
    "                h = tf.nn.dropout(h, keep_prob)\n",
    "            Ws.append(W)\n",
    "            current_input = h\n",
    "\n",
    "    shapes.append(current_input.get_shape().as_list())\n",
    "\n",
    "    with tf.variable_scope('variational'):\n",
    "        if variational:\n",
    "            dims = current_input.get_shape().as_list()\n",
    "            flattened = library.flatten(current_input)\n",
    "\n",
    "            if n_hidden:\n",
    "                h = library.linear(flattened, n_hidden, name='W_fc')[0]\n",
    "                h = activation(batch_norm(h, phase_train, 'fc/bn'))\n",
    "                if dropout:\n",
    "                    h = tf.nn.dropout(h, keep_prob)\n",
    "            else:\n",
    "                h = flattened\n",
    "\n",
    "            z_mu = library.linear(h, n_code, name='mu')[0]\n",
    "            z_log_sigma = 0.5 * library.linear(h, n_code, name='log_sigma')[0]\n",
    "\n",
    "            # Sample from noise distribution p(eps) ~ N(0, 1)\n",
    "            epsilon = tf.random_normal(\n",
    "                tf.stack([tf.shape(x)[0], n_code]))\n",
    "\n",
    "            # Sample from posterior\n",
    "            z = z_mu + tf.multiply(epsilon, tf.exp(z_log_sigma))\n",
    "\n",
    "            if n_hidden:\n",
    "                h = library.linear(z, n_hidden, name='fc_t')[0]\n",
    "                h = activation(batch_norm(h, phase_train, 'fc_t/bn'))\n",
    "                if dropout:\n",
    "                    h = tf.nn.dropout(h, keep_prob)\n",
    "            else:\n",
    "                h = z\n",
    "\n",
    "            size = dims[1] * dims[2] * dims[3] if convolutional else dims[1]\n",
    "            h = library.linear(h, size, name='fc_t2')[0]\n",
    "            current_input = activation(batch_norm(h, phase_train, 'fc_t2/bn'))\n",
    "            if dropout:\n",
    "                current_input = tf.nn.dropout(current_input, keep_prob)\n",
    "\n",
    "            if convolutional:\n",
    "                current_input = tf.reshape(\n",
    "                    current_input, tf.stack([\n",
    "                        tf.shape(current_input)[0],\n",
    "                        dims[1],\n",
    "                        dims[2],\n",
    "                        dims[3]]))\n",
    "        else:\n",
    "            z = current_input\n",
    "\n",
    "    shapes.reverse()\n",
    "    n_filters.reverse()\n",
    "    Ws.reverse()\n",
    "\n",
    "    n_filters += [input_shape[-1]]\n",
    "\n",
    "    # %%\n",
    "    # Decoding layers\n",
    "    for layer_i, n_output in enumerate(n_filters[1:]):\n",
    "        with tf.variable_scope('decoder/{}'.format(layer_i)):\n",
    "            shape = shapes[layer_i + 1]\n",
    "            if convolutional:\n",
    "                h, W = library.deconv2d(x=current_input,\n",
    "                                      n_output_h=shape[1],\n",
    "                                      n_output_w=shape[2],\n",
    "                                      n_output_ch=shape[3],\n",
    "                                      n_input_ch=shapes[layer_i][3],\n",
    "                                      k_h=filter_sizes[layer_i],\n",
    "                                      k_w=filter_sizes[layer_i])\n",
    "            else:\n",
    "                h, W = library.linear(x=current_input,\n",
    "                                    n_output=n_output)\n",
    "            h = activation(batch_norm(h, phase_train, 'dec/bn' + str(layer_i)))\n",
    "            if dropout:\n",
    "                h = tf.nn.dropout(h, keep_prob)\n",
    "            current_input = h\n",
    "\n",
    "    y = current_input\n",
    "    x_flat = library.flatten(x)\n",
    "\n",
    "    # l2 loss\n",
    "    loss_x = tf.reduce_sum(tf.squared_difference(x_flat, y_flat), 1)\n",
    "\n",
    "    if variational:\n",
    "        # variational lower bound, kl-divergence\n",
    "        loss_z = -0.5 * tf.reduce_sum(\n",
    "            1.0 + 2.0 * z_log_sigma -\n",
    "            tf.square(z_mu) - tf.exp(2.0 * z_log_sigma), 1)\n",
    "\n",
    "        # add l2 loss\n",
    "        cost = tf.reduce_mean(loss_x + loss_z)\n",
    "    else:\n",
    "        # just optimize l2 loss\n",
    "        cost = tf.reduce_mean(loss_x)\n",
    "\n",
    "    return {'cost': cost, 'Ws': Ws,\n",
    "            'x': x, 'z': z, 'y': y,\n",
    "            'keep_prob': keep_prob,\n",
    "            'corrupt_prob': corrupt_prob,\n",
    "            'train': phase_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_vae(files,\n",
    "              input_shape,\n",
    "              learning_rate=0.0001,\n",
    "              batch_size=100,\n",
    "              n_epochs=50,\n",
    "              n_examples=10,\n",
    "              crop_shape=[64, 64, 3],\n",
    "              crop_factor=0.8,\n",
    "              n_filters=[100, 100, 100, 100],\n",
    "              n_hidden=256,\n",
    "              n_code=50,\n",
    "              convolutional=True,\n",
    "              variational=True,\n",
    "              filter_sizes=[3, 3, 3, 3],\n",
    "              dropout=True,\n",
    "              keep_prob=0.8,\n",
    "              activation=tf.nn.relu,\n",
    "              img_step=100,\n",
    "              save_step=100,\n",
    "              ckpt_name=\"vae.ckpt\"):\n",
    "    \"\"\"General purpose training of a (Variational) (Convolutional) Autoencoder.\n",
    "\n",
    "    Supply a list of file paths to images, and this will do everything else.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list of strings\n",
    "        List of paths to images.\n",
    "    input_shape : list\n",
    "        Must define what the input image's shape is.\n",
    "    learning_rate : float, optional\n",
    "        Learning rate.\n",
    "    batch_size : int, optional\n",
    "        Batch size.\n",
    "    n_epochs : int, optional\n",
    "        Number of epochs.\n",
    "    n_examples : int, optional\n",
    "        Number of example to use while demonstrating the current training\n",
    "        iteration's reconstruction.  Creates a square montage, so make\n",
    "        sure int(sqrt(n_examples))**2 = n_examples, e.g. 16, 25, 36, ... 100.\n",
    "    crop_shape : list, optional\n",
    "        Size to centrally crop the image to.\n",
    "    crop_factor : float, optional\n",
    "        Resize factor to apply before cropping.\n",
    "    n_filters : list, optional\n",
    "        Same as VAE's n_filters.\n",
    "    n_hidden : int, optional\n",
    "        Same as VAE's n_hidden.\n",
    "    n_code : int, optional\n",
    "        Same as VAE's n_code.\n",
    "    convolutional : bool, optional\n",
    "        Use convolution or not.\n",
    "    variational : bool, optional\n",
    "        Use variational layer or not.\n",
    "    filter_sizes : list, optional\n",
    "        Same as VAE's filter_sizes.\n",
    "    dropout : bool, optional\n",
    "        Use dropout or not\n",
    "    keep_prob : float, optional\n",
    "        Percent of keep for dropout.\n",
    "    activation : function, optional\n",
    "        Which activation function to use.\n",
    "    img_step : int, optional\n",
    "        How often to save training images showing the manifold and\n",
    "        reconstruction.\n",
    "    save_step : int, optional\n",
    "        How often to save checkpoints.\n",
    "    ckpt_name : str, optional\n",
    "        Checkpoints will be named as this, e.g. 'model.ckpt'\n",
    "    \"\"\"\n",
    "    batch = create_input_pipeline(\n",
    "        files=files,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=n_epochs,\n",
    "        crop_shape=crop_shape,\n",
    "        crop_factor=crop_factor,\n",
    "        shape=input_shape)\n",
    "\n",
    "    ae = VAE(input_shape=[None] + crop_shape,\n",
    "             convolutional=convolutional,\n",
    "             variational=variational,\n",
    "             n_filters=n_filters,\n",
    "             n_hidden=n_hidden,\n",
    "             n_code=n_code,\n",
    "             dropout=dropout,\n",
    "             filter_sizes=filter_sizes,\n",
    "             activation=activation)\n",
    "\n",
    "    # Create a manifold of our inner most layer to show\n",
    "    # example reconstructions.  This is one way to see\n",
    "    # what the \"embedding\" or \"latent space\" of the encoder\n",
    "    # is capable of encoding, though note that this is just\n",
    "    # a random hyperplane within the latent space, and does not\n",
    "    # encompass all possible embeddings.\n",
    "    zs = np.random.uniform(\n",
    "        -1.0, 1.0, [4, n_code]).astype(np.float32)\n",
    "    zs = library.make_latent_manifold(zs, n_examples)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=learning_rate).minimize(ae['cost'])\n",
    "\n",
    "    # We create a session to use the graph\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # This will handle our threaded image pipeline\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "    # Ensure no more changes to graph\n",
    "    tf.get_default_graph().finalize()\n",
    "\n",
    "    # Start up the queues for handling the image pipeline\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    if os.path.exists(ckpt_name + '.index') or os.path.exists(ckpt_name):\n",
    "        saver.restore(sess, ckpt_name)\n",
    "\n",
    "    # Fit all training data\n",
    "    t_i = 0\n",
    "    batch_i = 0\n",
    "    epoch_i = 0\n",
    "    cost = 0\n",
    "    n_files = len(files)\n",
    "    test_xs = sess.run(batch) / 255.0\n",
    "    library.montage(test_xs, 'test_xs.png')\n",
    "    try:\n",
    "        while not coord.should_stop() and epoch_i < n_epochs:\n",
    "            batch_i += 1\n",
    "            batch_xs = sess.run(batch) / 255.0\n",
    "            train_cost = sess.run([ae['cost'], optimizer], feed_dict={\n",
    "                ae['x']: batch_xs, ae['train']: True,\n",
    "                ae['keep_prob']: keep_prob})[0]\n",
    "            print(batch_i, train_cost)\n",
    "            cost += train_cost\n",
    "            if batch_i % n_files == 0:\n",
    "                print('epoch:', epoch_i)\n",
    "                print('average cost:', cost / batch_i)\n",
    "                cost = 0\n",
    "                batch_i = 0\n",
    "                epoch_i += 1\n",
    "\n",
    "            if batch_i % img_step == 0:\n",
    "                # Plot example reconstructions from latent layer\n",
    "                recon = sess.run(\n",
    "                    ae['y'], feed_dict={\n",
    "                        ae['z']: zs,\n",
    "                        ae['train']: False,\n",
    "                        ae['keep_prob']: 1.0})\n",
    "                library.montage(recon.reshape([-1] + crop_shape),\n",
    "                              'manifold_%08d.png' % t_i)\n",
    "\n",
    "                # Plot example reconstructions\n",
    "                recon = sess.run(\n",
    "                    ae['y'], feed_dict={ae['x']: test_xs,\n",
    "                                        ae['train']: False,\n",
    "                                        ae['keep_prob']: 1.0})\n",
    "                print('reconstruction (min, max, mean):',\n",
    "                    recon.min(), recon.max(), recon.mean())\n",
    "                library.montage(recon.reshape([-1] + crop_shape),\n",
    "                              'reconstruction_%08d.png' % t_i)\n",
    "                t_i += 1\n",
    "\n",
    "            if batch_i % save_step == 0:\n",
    "                # Save the variables to disk.\n",
    "                saver.save(sess, \"./\" + ckpt_name,\n",
    "                           global_step=batch_i,\n",
    "                           write_meta_graph=False)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done.')\n",
    "    finally:\n",
    "        # One of the threads has issued an exception.  So let's tell all the\n",
    "        # threads to shutdown.\n",
    "        coord.request_stop()\n",
    "\n",
    "    # Wait until all threads have finished.\n",
    "    coord.join(threads)\n",
    "\n",
    "    # Clean up the session.\n",
    "    sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
